# background_stage1.py
"""
Stage 1: Background ingestion of SAP / Vault / PowerBI / PO / Invoice
â†’ Cleansing â†’ Merging by part_number â†’ Upsert into ONE flat table part_master.
"""

import os
from collections import defaultdict

import pandas as pd

from config import SOURCES_DIRS, OUTPUT_DIR
from ingestion_utils import load_file
from cleansing import cleanup_pipeline
from enrichment_text import enrich_from_description
from merge_logic import merge_records_by_part_number
from db import init_db, upsert_part_master


def load_all_sources() -> pd.DataFrame:
    all_rows = []
    for system, folder in SOURCES_DIRS.items():
        if not os.path.isdir(folder):
            continue
        for fname in os.listdir(folder):
            path = os.path.join(folder, fname)
            print(f"ğŸ“„ [{system}] Processing: {path}")
            df = load_file(path)
            if df is None or df.empty:
                continue
            df["source_system"] = system
            df["source_file"] = fname
            all_rows.append(df)
    if not all_rows:
        return pd.DataFrame()
    return pd.concat(all_rows, ignore_index=True)


def clean_pipeline(df: pd.DataFrame) -> pd.DataFrame:
    df = cleanup_pipeline(df)
    df = enrich_from_description(df)
    return df


def run_stage1():
    print("ğŸš€ Stage 1: Background Ingestion Started\n")

    try:
        init_db()
        print("âœ… Database initialized")

        df_raw = load_all_sources()
        print(f"ğŸ“Š Raw rows loaded: {len(df_raw)}")

        if df_raw.empty:
            print("âš ï¸  No data loaded from sources")
            return

        df_clean = clean_pipeline(df_raw)

        # Convert to records
        records = df_clean.to_dict(orient="records")

        # Group by part number
        grouped = {}
        for r in records:
            pn = r.get("part_number")
            if not pn:
                continue
            grouped.setdefault(pn, []).append(r)

        # Merge rows for each part
        merged_records = []
        for pn, rows in grouped.items():
            merged = merge_records_by_part_number(rows)
            merged_records.append(merged)

        print(f"ğŸ“Š Unique merged part_numbers: {len(merged_records)}")

        # Upsert
        if merged_records:
            upsert_part_master(merged_records)
            print(f"âœ… Upserted {len(merged_records)} records to database")
        else:
            print("âš ï¸  No records to upsert")

        # Save snapshot for inspection
        snapshot_path = os.path.join(OUTPUT_DIR, "stage1_master_snapshot.xlsx")
        pd.DataFrame(merged_records).to_excel(snapshot_path, index=False)
        print(f"ğŸ“‚ Snapshot saved to: {snapshot_path}")
        print("âœ… Stage 1 complete.")

    except Exception as e:
        print(f"âŒ Stage 1 failed: {e}")
        import traceback
        traceback.print_exc()
        raise


if __name__ == "__main__":
    run_stage1()